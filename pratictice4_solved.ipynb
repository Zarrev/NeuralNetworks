{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784) (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import ecery library what you need for a fully connected neural network\n",
    "# hint: 3 package + 1 for drawing nice things into the universe\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# As you know from the before classes you need a dataset, so please import\n",
    "# the MNIST dataset\n",
    "#hint: from, import\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "\n",
    "# Read the dataset into a variable and (pls) use the one hot label encoding\n",
    "# hint: one function calling with two parameters where the first is the\n",
    "# 'MNIST_data/' folder(?) and the sec param is one_hot=...\n",
    "data = mnist_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "# Print out the images' shape and the labels' shape\n",
    "# hint: you can find shape property in the data.train.whatyouneed.shape\n",
    "# print(data)\n",
    "print(data.train.images.shape, data.train.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"fully_1/weight:0\", shape=(784, 1024), dtype=float32_ref) must be from the same graph as Tensor(\"Mean:0\", shape=(), dtype=float32).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c170be2e73b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Definition of optimizer as a GradientDescentOptimizer with 1e-4 learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# rate for the minimization of loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtraining_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Calculaltion of correct predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         colocate_gradients_with_ops=colocate_gradients_with_ops)\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_GRAPH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    628\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m--> 630\u001b[0;31m                             gate_gradients, aggregation_method, stop_gradients)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)\u001b[0m\n\u001b[1;32m    669\u001b[0m   with ops.name_scope(\n\u001b[1;32m    670\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gradients\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       list(ys) + list(xs) + list(stop_gradients) + list(grad_ys)) as grad_scope:\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;31m# Get a uid for this call to gradients that can be used to help\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;31m# cluster ops for compilation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6021\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6022\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6023\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6024\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6025\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   5662\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5663\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5664\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5665\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5666\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   5598\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5599\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[0;32m-> 5600\u001b[0;31m                                                                 original_item))\n\u001b[0m\u001b[1;32m   5601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"fully_1/weight:0\", shape=(784, 1024), dtype=float32_ref) must be from the same graph as Tensor(\"Mean:0\", shape=(), dtype=float32)."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# I would like to ask you to plot one picture to me.\n",
    "# hint: reshape an image, because maybe it is too big or something.\n",
    "# Ok, we need just smaller picture, it can be 28x28.\n",
    "#img = np.reshape(data.train.images[0, :], (28, 28))\n",
    "#plt.figure()\n",
    "#plt.imshow(img)\n",
    "#plt.show()\n",
    "\n",
    "# Define two integer what are the input_size and output_classes\n",
    "# hint: check again the shapes\n",
    "input_size = data.train.images.shape[1]\n",
    "output_size = data.train.labels.shape[1]\n",
    "\n",
    "# Call the reset default graph method. You will be grateful later\n",
    "# hint: tensorflow's function\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Save out into two variables the input images and the labels\n",
    "# hint: 2 placeholders, float32, predefine the shapes\n",
    "inputs = tf.placeholder(tf.float32, name=\"input\",\n",
    "                       shape=[None, input_size])\n",
    "labels = tf.placeholder(tf.float32, name=\"output\",\n",
    "                        shape=[None, output_size])\n",
    "\n",
    "# Write two hidden layer. First contains 1024 and the second\n",
    "# contains 2048 neurons. Use relu aas activation function.\n",
    "# hint: variable_scope, weight, bias, matmul, add, nn.relu\n",
    "first_layer_neurons = 1024\n",
    "second_layer_neurons = 2048\n",
    "with tf.variable_scope('fully_1'):\n",
    "    w = tf.get_variable('weight', [input_size, first_layer_neurons])\n",
    "    b = tf.get_variable('bias', [first_layer_neurons])\n",
    "    \n",
    "    mult = tf.matmul(inputs, w)\n",
    "    biased = tf.add(mult, b)\n",
    "    \n",
    "    next_inp = tf.nn.relu(biased)\n",
    "\n",
    "with tf.variable_scope('fully_2'):\n",
    "    w = tf.get_variable('weight', [first_layer_neurons, second_layer_neurons])\n",
    "    b = tf.get_variable('bias', [second_layer_neurons])\n",
    "    \n",
    "    mult = tf.matmul(next_inp, w)\n",
    "    biased = tf.add(mult, b)\n",
    "    \n",
    "    next_inp = tf.nn.relu(biased) \n",
    "    \n",
    "# Write the output layer for me *-*. I know you know but I must say it\n",
    "# again... You don't have to use the activation function in this layer, \n",
    "# because it is your result.\n",
    "with tf.variable_scope('output_layer'):\n",
    "    w = tf.get_variable('weight', [second_layer_neurons, output_size])\n",
    "    b = tf.get_variable('bias', [output_size])\n",
    "    \n",
    "    mult = tf.matmul(next_inp, w)\n",
    "    biased = tf.add(mult, b)\n",
    "    \n",
    "final_output = biased\n",
    "    \n",
    "# Calculate loss with the help of softmax_cross_entropy that contains \n",
    "# the execution of softmax function on the output and the cross entropy \n",
    "# calculation between output and one_hot_labels\n",
    "# hint: reduce_mean, losses.softmax...\n",
    "losses = tf.reduce_mean(tf.losses.softmax_cross_entropy(labels, final_output),\n",
    "                        name= 'losses')\n",
    "\n",
    "# Definition of optimizer as a GradientDescentOptimizer with 1e-4 learning\n",
    "# rate for the minimization of loss\n",
    "training_steps = tf.train.GradientDescentOptimizer(1e-4).minimize(losses)\n",
    "\n",
    "# Calculaltion of correct predictions\n",
    "# hint: equal, argmax\n",
    "correct_prediction = tf.equal(tf.argmax(final_output), tf.argmax(labels),\n",
    "                              name='cor_pred')\n",
    "\n",
    "# Calculation of accuracy\n",
    "# hint: reduce_mean, cast\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),\n",
    "                         name='acc')\n",
    "\n",
    "# Write the run section where the nn will learn and it will get back\n",
    "# the accuracy and the loss. Print out the step, loss accuracy together\n",
    "# in every 100th steps. After the loop print out the acc and loss onto\n",
    "# a figure.\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as ss:\n",
    "    ss.run(init)\n",
    "    \n",
    "    batch_length = 64\n",
    "    num_iteration = 15000\n",
    "    \n",
    "    loss_plot = np.zeros(num_iteration)\n",
    "    acc_plot = np.zeros(num_iteration)\n",
    "    \n",
    "    for index in range(num_iteration):\n",
    "        used_in_batch = random.sample(range(data.train.images.shape[0]),\n",
    "                                      batch_length)\n",
    "        batch_in = data.train.images[used_in_batch, :]\n",
    "        batch_out = data.train.labels[used_in_batch, :]\n",
    "        \n",
    "        _, acc, loss = ss.run([training_steps, accuracy, losses], feed_dict={\n",
    "            inputs: batch_in,\n",
    "            labels: batch_out\n",
    "        })\n",
    "        \n",
    "        acc_plot[index] = acc\n",
    "        loss_plot[index] = loss\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            print('Step: ' + str(index) + 'Loss: ' + str(loss) \n",
    "                  + 'Acc: ' + str(acc))\n",
    "            \n",
    "plt.plot(acc_plot, 'b', loss_plot, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
