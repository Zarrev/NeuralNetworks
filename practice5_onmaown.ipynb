{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It makes the titanic.csv file into a usable dataset for our neural network\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Now we need to analyze our data, come to terms with the features it contains, and identify the features relevant to us. This part is called feature engineering.\n",
    "\n",
    "We are using the pandas package to create a pandas DataFrame which will contain our data.\n",
    "\"\"\"\n",
    "\n",
    "data = pd.read_csv('src_5/titanic.csv')\n",
    "\n",
    "data.head()\n",
    "\n",
    "\"\"\"We need to preprocess the data as part of feature engineering. The first step is to identify which features are irrelevant to us, therefore we can discard them.\n",
    "\n",
    "To make ValueError exceptions less likely to occur, we will replace all NaN values in our data with zeros.\n",
    "\"\"\"\n",
    "\n",
    "del data['Name']\n",
    "del data['Ticket']\n",
    "del data['Fare']\n",
    "del data['Embarked']\n",
    "\n",
    "data = data.fillna(value=0.0)\n",
    "\n",
    "data.head()\n",
    "\n",
    "\"\"\"Now we need to convert all our features to numerical, because our neural network is solely based on mathematical and statistical principles. For the sake of simplicity, we will also convert some data to make them representable by one-hot encoded vectors.\n",
    "After this conversion, we will encode features with one-hot encoding that can be represented using a fixed set of values.\n",
    "\n",
    "One-hot representation basically is an array of zeros, with ones representing what the data is. For example, gender (speaking in traditional terms) can be represented by two values: male and female. Therefore we can represent this using an array with a length of 2:\n",
    "\n",
    "```\n",
    "male: [0, 1]\n",
    "female: [1, 0]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    if data.at[i, 'Sex'] == 'male':\n",
    "        data.at[i, 'Sex'] = 1\n",
    "    elif data.at[i, 'Sex'] == 'female':\n",
    "        data.at[i, 'Sex'] = 0\n",
    "\n",
    "data['Age_group'] = 0\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(70, 0, -10):\n",
    "        if data.at[i, 'Age'] > j:\n",
    "            data.at[i, 'Age_group'] = int(j / 10)\n",
    "            break\n",
    "\n",
    "del data['Age']\n",
    "\n",
    "data['Cabin_section'] = '0'\n",
    "for i in range(data.shape[0]):\n",
    "    if data.at[i, 'Cabin'] != 0:\n",
    "        data.at[i, 'Cabin_section'] = data.at[i, 'Cabin'][0]\n",
    "cabin_sections = list(set(data['Cabin_section'].values))\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    data.at[i, 'Cabin_section'] = cabin_sections.index(data.at[i, 'Cabin_section'])\n",
    "\n",
    "del data['Cabin']\n",
    "\n",
    "pclass = np.eye(data['Pclass'].values.max() + 1)[data['Pclass'].values]\n",
    "age_group = np.eye(data['Age_group'].values.max() + 1)[data['Age_group'].values]\n",
    "cabin_section = np.eye(data['Cabin_section'].values.max() + 1)[data['Cabin_section'].values.astype(int)]\n",
    "sex = np.eye(data['Sex'].values.max() + 1)[data['Sex'].values.astype(int)]\n",
    "\n",
    "train_data = data[['SibSp', 'Parch']].values\n",
    "train_data = np.concatenate([train_data, sex], axis=1)\n",
    "train_data = np.concatenate([train_data, age_group], axis=1)\n",
    "train_data = np.concatenate([train_data, pclass], axis=1)\n",
    "train_data = np.concatenate([train_data, cabin_section], axis=1)\n",
    "train_data = train_data.astype(float)\n",
    "\n",
    "train_labels = data['Survived'].values\n",
    "train_labels = train_labels.astype(float).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 25)\n",
      "(891, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_data.shape[-1]\n",
    "label_size = train_labels.shape[-1]\n",
    "all_input_size = train_data.shape[0]\n",
    "num_iter = 9000\n",
    "learning_rate = 1e-4\n",
    "num_kernels = [32, 64, 128, label_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "my_input = tf.placeholder(tf.float32, [None] + [input_size])\n",
    "my_out = tf.placeholder(tf.float32, [None] + [label_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layer(scope_name, layer_num, input_s, outp_s, input_data, output_layer=False):\n",
    "    with tf.variable_scope(scope_name + str(layer_num)):\n",
    "        w = tf.get_variable('w', [input_s, outp_s])\n",
    "        b = tf.get_variable('b', [outp_s])\n",
    "        out = tf.add(tf.matmul(input_data, w), b)\n",
    "        if output_layer is not True:\n",
    "            out = tf.nn.relu(out)\n",
    "            \n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up computional graph\n",
    "current_input = my_input\n",
    "current_kernel = input_size\n",
    "for num, kernel in enumerate(num_kernels):\n",
    "    if num_kernels[-1] != kernel:\n",
    "        current_input = create_layer('fullyconnected', num, current_kernel, kernel, current_input)\n",
    "    else:\n",
    "        current_input = create_layer('fullyconnected_output', num, current_kernel, kernel, current_input, True)\n",
    "    current_kernel = kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss, training step, accuracy\n",
    "def get_loss_opt_acc(labels, output, learning_rate, fire=0.5):\n",
    "    with tf.variable_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=output))\n",
    "    with tf.variable_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    with tf.variable_scope('accuracy'):\n",
    "        predicton = tf.cast(tf.greater_equal(output, fire), tf.float32)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predicton, labels), tf.float32))\n",
    "        \n",
    "    return [loss, optimizer, accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0941432 acc: 0.25\n",
      "loss: 0.5084117 acc: 0.6875\n",
      "loss: 0.43963253 acc: 0.78125\n",
      "loss: 0.3075295 acc: 0.9375\n",
      "loss: 0.5227163 acc: 0.75\n",
      "loss: 0.33335707 acc: 0.75\n",
      "loss: 0.3525663 acc: 0.84375\n",
      "loss: 0.36258075 acc: 0.8125\n",
      "loss: 0.23073527 acc: 0.875\n",
      "loss: 0.4040806 acc: 0.84375\n",
      "loss: 0.5250921 acc: 0.71875\n",
      "loss: 0.34017962 acc: 0.8125\n",
      "loss: 0.29173243 acc: 0.875\n",
      "loss: 0.3320443 acc: 0.90625\n",
      "loss: 0.23557481 acc: 0.875\n",
      "loss: 0.3501238 acc: 0.90625\n",
      "loss: 0.23566785 acc: 0.90625\n",
      "loss: 0.26321542 acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "loss_opt_acc = get_loss_opt_acc(my_out, current_input, learning_rate)\n",
    "acc_plot = np.zeros(num_iter)\n",
    "loss_plot = np.zeros(num_iter)\n",
    "with tf.Session() as ss:\n",
    "    ss.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        batch_in_used = random.sample(range(train_data.shape[0]), batch_size)\n",
    "        xs_batch = train_data[batch_in_used]\n",
    "        ys_batch = train_labels[batch_in_used]\n",
    "        \n",
    "        #for j in range(batch_size):\n",
    "        loss, _, acc = ss.run(loss_opt_acc, feed_dict={my_input: xs_batch, my_out: ys_batch})\n",
    "        acc_plot[i] = acc\n",
    "        loss_plot[i] = loss\n",
    "            \n",
    "        if i % 500 == 0:\n",
    "            print('loss: ' + str(loss) + ' ' + 'acc: ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_plot, 'r', acc_plot, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
