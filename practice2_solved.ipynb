{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First of all import tensorflow (always)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_25:0\", shape=(), dtype=int32)\nTensor(\"Const_26:0\", shape=(), dtype=float32)\nTensor(\"Const_27:0\", shape=(2, 3), dtype=float32)\n3\n3.0\n[[1. 2. 3.]\n [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Let's make some constants! This is the most basic operation. \n",
    "# The creation of constant itself is the most basic operation!\n",
    "\n",
    "# Make a constant that contains 3 and make a constant that contains 3.0 and the type of it is float32.\n",
    "x = tf.constant(3)\n",
    "y = tf.constant(3.0, dtype=tf.float32)\n",
    "\n",
    "# Make a constant list and matrix.\n",
    "# Give a new shape to them.\n",
    "const_list = tf.constant([1, 2, 3, 4, 5, 6], dtype=tf.float32, shape=(2, 3))\n",
    "\n",
    "# If we want to print out the constants then we will get the tensors \n",
    "# but we can't see values because we just built the computional graph.\n",
    "# We defined the operations but we did not run them. These objects\n",
    "# just represent the results of the operations that will be run.\n",
    "# Try to print it and check the magic!\n",
    "print(x)\n",
    "print(y)\n",
    "print(const_list)\n",
    "\n",
    "# So run them! (Just run away from this....)\n",
    "session = tf.Session()\n",
    "print(session.run(x))\n",
    "print(session.run(y))\n",
    "print(session.run(const_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We can use other kind of operations like:\n",
    "#\n",
    "#\n",
    "# *   tf.add\n",
    "# *   tf.multiply\n",
    "# *   tf.divide\n",
    "# *   etc\n",
    "#\n",
    "# Let's see how the computional graphs are changing in the background. Try it!\n",
    "\n",
    "z = tf.add(y,tf.divide(y,0.2))\n",
    "print(session.run(z))\n",
    "z = tf.multiply(const_list,z)\n",
    "print(session.run(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n3.0\n6.0\n9.0\n"
     ]
    }
   ],
   "source": [
    "# Now we have 3 node in the:[computional graph] and the result will be in c\n",
    "# (https://drive.google.com/open?id=19P22lK_eiXlHM20yy3RraoQto3IVMxcS)\n",
    "# Write few line of code what can represent this figure (link above).\n",
    "# Feel free to choose the value of the constants.\n",
    "a = tf.constant(3.0)\n",
    "b = tf.constant(3.0)\n",
    "c = tf.add(a,b)\n",
    "\n",
    "# Run the session to calculate the result of computional graph:\n",
    "print(session.run(c))\n",
    "\n",
    "# Build a more complex graph: make 4 constant and define the operations between them\n",
    "aa = tf.constant(2.0)\n",
    "bb = tf.constant(1.0)\n",
    "cc = tf.constant(3.0)\n",
    "dd = tf.constant(4.0)\n",
    "xx = tf.subtract(dd, bb)\n",
    "yy = tf.multiply(aa,cc)\n",
    "result = tf.add(xx,yy)\n",
    "# Run it as a session: show me how would you do it with the 'with' keyword\n",
    "with tf.Session() as ss:\n",
    "    print(ss.run(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Placeholders: These are like parameters in the computional graph. \n",
    "# A placeholder is a promise to provide a value later, like a \n",
    "# function argument.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Declaration of placeholders:declare two placeholder with the ordinary type\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Define a computional graph: What does it mean? \n",
    "# You should define the operation between the created placeholders\n",
    "z = tf.add(x, y)\n",
    "\n",
    "# Define the values of placeholders in a dictionary: \n",
    "# In this case you give values to your tensors. Just do it!\n",
    "feed_dict = {x: 8.0, y: 2.0}\n",
    "\n",
    "# Run it as a session and give in the feed_dictionary in order to load \n",
    "# placeholders\n",
    "with tf.Session() as ss:\n",
    "    print(ss.run(z,feed_dict))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.999997 7.999997]]\n"
     ]
    }
   ],
   "source": [
    "# Solving equation with tensorflow:\n",
    "\n",
    "# Prdefined inputs for the exercise\n",
    "x = tf.constant([[5., 6.]])\n",
    "y = tf.Variable(tf.zeros([1, 2])) # Initialize it with zero vector\n",
    "z = tf.constant([[2.]])\n",
    "f = tf.constant([[24., 28.]])\n",
    "\n",
    "# Define the equation : (x+y)*z = q\n",
    "q = tf.multiply(z,tf.add(x,y))\n",
    "\n",
    "# Define the squared deviation\n",
    "sdev = tf.square(f-q)\n",
    "\n",
    "# Define the training step: \n",
    "# use an optimezer and a the minimize function for it \n",
    "# The optimizer needs a learning rate\n",
    "# The minimizer needs the deviation as a parameter too.\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(sdev)\n",
    "\n",
    "# Define the Op that initializes global variables in the graph\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run it! Here as usual you have to use the Session with a for-loop for it.\n",
    "# First of all, determine an iteration number\n",
    "\n",
    "iter = 5000\n",
    "with tf.Session() as ss:\n",
    "    ss.run(init)\n",
    "    for e in range(iter):\n",
    "        ss.run(train_step)\n",
    "    print(ss.run(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a method that will be able to execute linear regression:\n",
    "# Import that library what need for the plotting first!\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# Write a function with the following paramters:\n",
    "# function name = simple_linear_regression\n",
    "def simple_linear_regression():\n",
    "\n",
    "    # Definition of training data:\n",
    "    train_x = [3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n",
    "            7.042, 10.791, 5.313, 7.997, 5.654, 9.27,3.1, 2.0, 2.45, 5.0]\n",
    "    train_y = [1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n",
    "          1.221,2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3, 3.0, 4.2, 4.8]\n",
    "    \n",
    "    # Declaration of placeholders for the training datas\n",
    "    x = tf.placeholder(tf.float32)\n",
    "    y = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Definition of variables that will contain the weight vector and\n",
    "    # bias vector that are related to the given linear regression problem\n",
    "    w = tf.Variable([1.0], name=\"weight\")\n",
    "    b = tf.Variable([1.0], name=\"bias\")\n",
    "    \n",
    "    # build the comp graph: so define the operations between the variables\n",
    "    pred = tf.multiply(w,x)\n",
    "    pred = tf.add(w,b)\n",
    "    \n",
    "    # define the loss function like: square, subtract\n",
    "    loss = tf.square(tf.subtract(y,pred))\n",
    "    \n",
    "    # define the training step like previously: optimizer, minimizer\n",
    "    opt = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "    \n",
    "    #Write the global variables' initializer and the session\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Let's traint it! Use epochs with a range and after that\n",
    "    # Execute a training per each point of the training set:\n",
    "    # So run the session with the optimizer and the training datas in a dict\n",
    "    # After that calculate the w_opt and the b_opt\n",
    "    # After that show me the what did you get after the for-loop (print)\n",
    "    # Create a plot with a figure and its axises\n",
    "    with tf.Session() as ss:\n",
    "        ss.run(init)\n",
    "        for epoch in range(1000):\n",
    "            for i in range(len(train_x)):\n",
    "                _ = ss.run(loss, feed_dict={x: train_x[i], y: train_y[i]})\n",
    "        fig, ax = plt.subplots()\n",
    "        w_opt = ss.run(w)\n",
    "        b_opt = ss.run(b)\n",
    "    # let's calculate our predictions: make a list for prediction results\n",
    "    # Write a for-loop for it with your input data's length,\n",
    "    # where you will insert your results into your prediction list\n",
    "    prediction = []\n",
    "    for i in range(len(train_x)):\n",
    "        prediction.append(train_x * w_opt + b_opt)\n",
    "    \n",
    "    # Disply the result with the ploting's function\n",
    "    ax.plot(train_x, train_y, 'ro')\n",
    "    ax.plot(train_x, prediction, 'bo')\n",
    "    plt.show()\n",
    "    \n",
    "simple_linear_regression() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
